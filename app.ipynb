{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "import os,asyncio,warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = [file for file in os.listdir('data') if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "pages = PyPDFLoader(file_path=f'/home/faizan/Downloads/Get_Started_With_Smallpdf.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatteningdocs(data):\n",
    "    return [item for sublist in data for item in sublist]\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "def get_data(file_path):\n",
    "    pages = PyPDFLoader(file_path=f'data/{file_path}').load()\n",
    "    if len(pages) > 1:\n",
    "        pdfstring = \"\"\n",
    "        metadata = {}\n",
    "        for page in pages:\n",
    "            pdfstring += page.page_content\n",
    "            metadata.update(page.metadata)\n",
    "\n",
    "        return [Document(\n",
    "            page_content=pdfstring,\n",
    "            metadata=metadata)]\n",
    "        \n",
    "    else:\n",
    "        return pages\n",
    "\n",
    "content_chain = RunnablePassthrough() | get_data\n",
    "content = flatteningdocs(await content_chain.abatch(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('UpdatedResumeDataSet.csv')\n",
    "# resumes = df['Resume'].tolist()\n",
    "resumes = df[(df['Category'] == \"Database\") | (df['Category'] == \"Data Science\") | (df['Category'] == 'Python Developer')]\n",
    "\n",
    "resumes.drop_duplicates(inplace=True)\n",
    "resumes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "data_tuples = [tuple(x) for x in resumes.to_records(index=False)]\n",
    "\n",
    "def save_to_pdf(data):\n",
    "    for index,(category, resume_text) in enumerate(data):\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font('Arial', size=10)\n",
    "        pdf.set_title(f\"{category}.pdf\")\n",
    "        \n",
    "        # Encode resume_text to 'latin-1' to handle Unicode characters\n",
    "        try:\n",
    "            encoded_text = resume_text.encode('latin-1', 'replace').decode('latin-1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding text for {category}.pdf: {e}\")\n",
    "            continue\n",
    "        \n",
    "        pdf.multi_cell(0, 10, encoded_text, border=1)\n",
    "        \n",
    "        # Save the PDF file\n",
    "        pdf_file_name = f\"data/{category}{index + 1}.pdf\"\n",
    "        pdf.output(pdf_file_name)\n",
    "\n",
    "save_to_pdf(data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "# vector_store = Chroma.from_texts(embedding=embedding,texts=resumes)\n",
    "persist_directory = \"database\"\n",
    "vector_store = Chroma.from_documents(embedding=embedding,documents=content,persist_directory=persist_directory)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_kwargs={'k':20})\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "        # search_kwargs={'score_threshold': 0.1,'k':30})\n",
    "# print(len(getunique(retriever.invoke(des))))\n",
    "len(retriever.invoke(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'source': 'data/Data Science8.pdf'}, page_content='Education Details \\n B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology\\nData Science \\nData Science\\nSkill Details \\nNumpy- Exprience - Less than 1 year months\\nMachine Learning- Exprience - Less than 1 year months\\nTensorflow- Exprience - Less than 1 year months\\nScikit- Exprience - Less than 1 year months\\nPython- Exprience - Less than 1 year months\\nGCP- Exprience - Less than 1 year months\\nPandas- Exprience - Less than 1 year months\\nNeural Network- Exprience - Less than 1 year monthsCompany Details \\ncompany - Wipro\\ndescription - Bhawana Aggarwal\\nE-Mail:bhawana.chd@gmail.com\\nPhone: 09876971076\\nVVersatile, high-energy professional targeting challenging assignments in Machine\\nPROFILE SUMMARY\\nâ–ª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine\\nLearning, Deep Learning, Data Science, Python, Software Development.\\nâ–ª Skilled in managing end-to-end development and software products / projects from inception, requirement\\nspecs, planning, designing, implementation, configuration and documentation.\\nâ–ª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,\\nNLP, GCP.â–ª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.\\nâ–ª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,\\nSupport vector Machine(SVM),Logistic Regression, Neural networks.\\nâ–ª Have knowledge on unsupervised, Supervised and reinforcement data.\\nâ–ª Programming experience in relational platforms like MySQL,Oracle.\\nâ–ª Have knowledge on Some programming language like C++,Java.\\nâ–ª Experience in cloud based environment like Google Cloud.\\nâ–ª Working on different Operating System like Linux, Ubuntu, Windows.\\nâ–ª Good interpersonal and communication skills.\\nâ–ª Problem solving skills with the ability to think laterally, and to think with a medium term and long term\\nperspective\\nâ–ª Flexibility and an open attitude to change.\\nâ–ª Ability to create, define and own frameworks with a strong emphasis on code reusability.\\nTECHNICAL SKILLS\\nProgramming Languages Python, C\\nLibraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib\\nAlgorithms\\nKNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,\\nTensorflow, SVM\\nDatabases SQL, Oracle\\nOperating Systems Linux, Window\\nDevelopment Environments NetBeans, Notebooks, Sublime\\nTicketing tools Service Now, Remedy\\nEducation\\nUG Education:\\nB.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in2016.\\nSchooling:\\nXII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)\\nX in 2010 from Valley Public School (Passed with 9.4 CGPA)\\nWORK EXPERINCE\\nTitle : Wipro Neural Intelligence Platform\\nTeam Size : 5\\nBrief: Wiproâ€™s Neural Intelligence Platform harnesses the power of automation and artificial intelligence\\ntechnologiesâ€”natural language processing (NLP), cognitive, machine learning, and analytics. The platform\\ncomprises three layers: a data engagement platform that can easily access and manage multiple structured and\\nunstructured data sources; an â€œintent assessment and reasoningâ€\\x9d engine that includes sentiment and predictive\\nanalytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed\\nautomating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,\\nNLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be\\neliminated.\\nEntity Extractor -> This involves text extraction and NLP for fetching out important information from the text like\\ndates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâ€™s and machine learning using\\nTensor flow for further learning of new entities.\\nClassifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn\\nclassifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best\\nsuited response and make the system efficient.\\nNER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,\\nOrganizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities\\nusing Keras TensorFlow framework.\\nOTHER PROJECTS\\nTitle : Diabetes DetectionBrief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third\\nprize in it.\\nTRAINING AND CERTIFICATIONS\\nTitle: Python Training, Machine Learning, Data Science, Deep Learning\\nOrganization: Udemy, Coursera (Machine Learning, Deep Learning)\\nPersonal Profile\\nFatherâ€™s Name :Mr. Tirlok Aggarwal\\nLanguage Known : English & Hindi\\nMarital Status :Single\\nDate of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)\\ncompany - Wipro\\ndescription - Developing programs in Python.\\ncompany - Wipro\\ndescription - Title : Wipro Neural Intelligence Platform\\nTeam Size : 5\\nBrief: Wiproâ€™s Neural Intelligence Platform harnesses the power of automation and artificial intelligence\\ntechnologiesâ€”natural language processing (NLP), cognitive, machine learning, and analytics. The platform\\ncomprises three layers: a data engagement platform that can easily access and manage multiple structured and\\nunstructured data sources; an â€œintent assessment and reasoningâ€\\x9d engine that includes sentiment and predictive\\nanalytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed\\nautomating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,\\nNLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be\\neliminated.\\nEntity Extractor -> This involves text extraction and NLP for fetching out important information from the text like\\ndates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâ€™s and machine learning using\\nTensor flow for further learning of new entities.Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn\\nclassifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best\\nsuited response and make the system efficient.\\nNER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,\\nOrganizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities\\nusing Keras TensorFlow framework.\\ncompany - Wipro Technologies\\ndescription - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine\\nLearning, Deep Learning, Data Science, Python, Software Development.\\nâ–ª Skilled in managing end-to-end development and software products / projects from inception, requirement\\nspecs, planning, designing, implementation, configuration and documentation.\\nâ–ª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,\\nNLP, GCP.\\nâ–ª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.\\nâ–ª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,\\nSupport vector Machine(SVM),Logistic Regression, Neural networks.\\nâ–ª Have knowledge on unsupervised, Supervised and reinforcement data.\\nâ–ª Programming experience in relational platforms like MySQL,Oracle.\\nâ–ª Have knowledge on Some programming language like C++,Java.\\nâ–ª Experience in cloud based environment like Google Cloud.\\nâ–ª Working on different Operating System like Linux, Ubuntu, Windows.\\nâ–ª Good interpersonal and communication skills.\\nâ–ª Problem solving skills with the ability to think laterally, and to think with a medium term and long term\\nperspective\\nâ–ª Flexibility and an open attitude to change.\\nâ–ª Ability to create, define and own frameworks with a strong emphasis on code reusability.'),\n",
       " Document(metadata={'page': 1, 'source': 'data/Data Science7.pdf'}, page_content='Skills â€¢ Python â€¢ Tableau â€¢ Data Visualization â€¢ R Studio â€¢ Machine Learning â€¢ Statistics IABAC\\nCertified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and\\nleading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of\\naugmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business\\nconcepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details \\nJanuary 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering\\nData Science Consultant \\nData Science Consultant - Datamites\\nSkill Details \\nMACHINE LEARNING- Exprience - 13 months\\nPYTHON- Exprience - 24 months\\nSOLUTIONS- Exprience - 24 months\\nDATA SCIENCE- Exprience - 24 months\\nDATA VISUALIZATION- Exprience - 24 months\\nTableau- Exprience - 24 monthsCompany Details \\ncompany - Datamites\\ndescription - â€¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.\\nâ€¢ Responsible for loading, extracting and validation of client data.\\nâ€¢ Worked on manipulating, cleaning & processing data using python.\\nâ€¢ Used Tableau for data visualization.\\ncompany - Heretic Solutions Pvt Ltd\\ndescription - â€¢ Worked closely with business to identify issues and used data to propose solutions for effective\\ndecision making.\\nâ€¢ Manipulating, cleansing & processing data using Python, Excel and R.\\nâ€¢ Analyzed raw data, drawing conclusions & developing recommendations.â€¢ Used machine learning tools and statistical techniques to produce solutions to problems.'),\n",
       " Document(metadata={'page': 1, 'source': 'data/Data Science9.pdf'}, page_content='Personal Skills âž¢ Ability to quickly grasp technical aspects and willingness to learn âž¢ High energy levels & Result\\noriented. Education Details \\nJanuary 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of\\nEngineering & Information Technology\\nJanuary 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of\\nEngineering\\nJanuary 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha\\nJanuary 2003 M.tech Thesis Detail  BMCH School in Ganj basoda\\nData science \\nI have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP,\\nPython, SQL, Web Scraping Good knowledge in computer subjects and ability to update\\nSkill Details \\nExperience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than\\n1 year monthsCompany Details \\ncompany - RNT.AI Technology Solution\\ndescription - Text classification using Machine learning Algorithms with python.\\nPractical knowledge of Deep learning algorithms such as Â\\xa0Recurrent Neural Networks(RNN).\\nDevelop custom data models and algorithms to apply to dataset\\nExperience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.\\nComfort with SQL, Â\\xa0MYSQL\\nSentiment analysis.\\nÂ\\xa0Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning\\nAlgorithms.\\nWeb crawling using Selenium web driver and Beautiful Soup with python.\\ncompany - Life Insurance Corporation of India Bhopaldescription - Ã¼Â\\xa0Explaining policy features and the benefits\\nÃ¼ Updated knowledge of life insurance products and shared with customers'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Data Science10.pdf'}, page_content='Expertise âˆ’ Data and Quantitative Analysis âˆ’ Decision Analytics âˆ’ Predictive Modeling âˆ’ Data-Driven\\nPersonalization âˆ’ KPI Dashboards âˆ’ Big Data Queries and Interpretation âˆ’ Data Mining and Visualization Tools âˆ’\\nMachine Learning Algorithms âˆ’ Business Intelligence (BI) âˆ’ Research, Reports and Forecasts Education Details \\n PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business\\n B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy\\nData Scientist \\nData Scientist with PR Canada\\nSkill Details \\nAlgorithms- Exprience - 6 months\\nBI- Exprience - 6 months\\nBusiness Intelligence- Exprience - 6 months\\nMachine Learning- Exprience - 24 months\\nVisualization- Exprience - 24 months\\nspark- Exprience - 24 months\\npython- Exprience - 36 months\\ntableau- Exprience - 36 months\\nData Analysis- Exprience - 24 monthsCompany Details \\ncompany - Aegis school of Data Science & Business\\ndescription - Mostly working on industry project for providing solution along with Teaching Appointments: Teach\\nundergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis\\nSchool of Data Science, Mumbai (2017 to Present)\\ncompany - Aegis school of Data & Business\\ndescription - Data Science Intern, Nov 2015 to Jan 2016\\nFurnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategicplanning across all business units, distribution channels and product lines.\\nâž” Chat Bot using AWS LEX and Tensor flow  Python\\nThe goal of project creates a chat bot for an academic institution or university to handle queries related courses offered\\nby that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this\\ncompanies handle their client 24x7. In this case companies are academic institutions and clients are participants or\\nstudents.\\nâž” Web scraping using Selenium web driver   Python\\nThe task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.\\nâž” Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense\\nThe goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which\\nis generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate\\na reports.\\nâž” Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV\\nThe main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing\\na special face from set of different faces. Face is extracted and then compared with the database Image if that Image\\nrecognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how\\nwe are going to prevent the frauds in the initial stage itself.\\nâž” Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop\\nThe objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer\\ngiving incentive offers.\\nâž” Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a\\nsentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions\\nof the tweets and comparative analysis.Quantifiable Results:\\nâˆ’ Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science\\nand Business Analytics.\\nâˆ’ Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference\\ncalled Data Science Congress by Aegis School of Business Mumbai.\\nâˆ’ Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai\\nand managed 4-5 live projects using Data Science techniques.\\nâˆ’ Working for some social cause with the help of Data Science for Social Goods Committee, where our team\\ndeveloped a product called \"Let\\'s find a missing Child\" for helping society.\\ncompany - IBM India pvt ltd\\ndescription - Mostly worked on blumix and IBM Watson for Data science.'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Data Science1.pdf'}, page_content='Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery.\\n* Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques,\\nCluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic\\nModelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase,\\nElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS,\\nAngular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep\\nlearning.Education Details \\nData Science Assurance Associate \\nData Science Assurance Associate - Ernst & Young LLP\\nSkill Details \\nJAVASCRIPT- Exprience - 24 months\\njQuery- Exprience - 24 months\\nPython- Exprience - 24 monthsCompany Details \\ncompany - Ernst & Young LLP\\ndescription - Fraud Investigations and Dispute Services   Assurance\\nTECHNOLOGY ASSISTED REVIEW\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery\\ndomain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor\\ncosts and time spent during the lawyers review.\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive\\nanalysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring\\nfor the entire tool.\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifiermodels in order to identify \"red flags\" and fraud-related issues.\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for\\ntopic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for\\npast one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments\\nacross all 4 categories.\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words\\nacross all the Survey categories and plotted Word cloud.\\n* Created customized tableau dashboards for effective reporting and visualizations.\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of\\noperation, reservation options and so on.\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give\\nrecommendation responses so that user question to build chain of relevant answer.\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant\\n/recommended questions.\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word\\nEmbedding, scikit-learn, JavaScript/JQuery, SqlServer\\nINFORMATION GOVERNANCE\\nOrganizations to make informed decisions about all of the information they store. The integrated Information\\nGovernance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure\\norganizations are best positioned to counter information risk.* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results\\nfor indexing elastic search and created customized, interactive dashboards using kibana.\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either\\nRedundant, Outdated, or Trivial.\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally\\nidentifiable information (social security numbers, addresses, names, etc.) which frequently targeted during\\ncyber-attacks.\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\nFRAUD ANALYTIC PLATFORM\\nFraud Analytics and investigative platform to review all red flag cases.\\nâ€¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP\\nsystems.\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators\\nof fraud by running advanced analytics\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"CV\"\n",
    "\n",
    "Groq = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\").with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\"))])\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\")).with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"dgoogle_api_key\"))]).with_fallbacks([Groq])\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "\n",
    "prompt_template = \"\"\"You are a powerfull assistant your task is to check weather the given CV match the given job requirements.Return only Yes if it match else return No.\n",
    "<job description>{question}</job description>\n",
    "# <cv>{context}</cv>\n",
    "> Relevant (YES / NO):\"\"\"\n",
    "\n",
    "def _get_default_chain_prompt() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        output_parser=BooleanOutputParser())\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(Groq,prompt=_get_default_chain_prompt())\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(des)\n",
    "\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.remove(f'shortlisted_cvs/{file}') for file in os.listdir('shortlisted_cvs')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # Install package\n",
    "# %pip install --upgrade --quiet \"unstructured[all-docs]\" --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('test',exist_ok=True)\n",
    "os.makedirs(\"test/faizan\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"test/faizan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_des': \"Company Description\\n \\nEvergreen Future Tech (EFT) is a software solutions provider located in Islamabad, Pakistan. We offer cutting-edge software solutions and specialize in artificial intelligence technologies.\\n\\n Role Description\\n \\nThis is a contract role for an Artificial Intelligence Engineer. The AI Engineer will be responsible for developing and implementing AI technologies, such as pattern recognition, neural networks, and natural language processing. The engineer will also be involved in software development and computer science-related tasks. This role is an on-site position located in Islamabad, Pakistan.\\n\\n Qualifications\\n \\nStrong background in computer science and software development\\nProficiency in pattern recognition and neural networks\\nExperience in natural language processing (NLP)\\nProblem-solving and analytical skills\\nExperience with AI frameworks and tools\\nKnowledge of machine learning algorithms\\nExcellent coding skills in languages such as Python or Java\\nStrong communication and collaboration skills\\nBachelor's or master's degree in computer science or a related field\",\n",
       " 'cv': \"**Name:** Ali Khan\\n\\n**Location:** Islamabad, Pakistan\\n\\n**Email:** ali.khan@example.com\\n\\n---\\n\\n**Professional Summary:**\\n\\nExperienced Artificial Intelligence Engineer with a strong background in computer science and software development. Proficient in pattern recognition, neural networks, and natural language processing. Adept at using various AI frameworks and tools to develop innovative solutions. Excellent problem-solving and analytical skills, coupled with strong communication and collaboration abilities.\\n\\n---\\n\\n**Work Experience:**\\n\\n**Artificial Intelligence Engineer**  \\n**Tech Innovators Pvt Ltd, Islamabad, Pakistan**  \\n**January 2020 - Present**\\n\\n- Developed and implemented AI technologies for various projects, including pattern recognition and neural networks.\\n- Designed and deployed NLP models for customer service chatbots, improving response accuracy by 30%.\\n- Collaborated with software development teams to integrate AI solutions into existing software systems.\\n- Utilized AI frameworks and tools such as TensorFlow and PyTorch for model development and deployment.\\n- Conducted research on machine learning algorithms to enhance the performance of AI models.\\n\\n**Software Developer**  \\n**Future Solutions Inc, Islamabad, Pakistan**  \\n**June 2017 - December 2019**\\n\\n- Worked on various software development projects, focusing on AI-related tasks.\\n- Developed and maintained code in Python and Java for different applications.\\n- Implemented machine learning algorithms to solve complex business problems.\\n- Collaborated with cross-functional teams to deliver high-quality software solutions.\\n\\n---\\n\\n**Education:**\\n\\n**Master's Degree in Computer Science**  \\n**National University of Sciences and Technology (NUST), Islamabad**  \\n**2015 - 2017**\\n\\n**Bachelor's Degree in Computer Science**  \\n**COMSATS Institute of Information Technology, Islamabad**  \\n**2011 - 2015**\\n\\n---\\n\\n**Skills:**\\n\\n- Artificial Intelligence (AI)\\n- Pattern Recognition\\n- Neural Networks\\n- Natural Language Processing (NLP)\\n- Machine Learning Algorithms\\n- Python, Java\\n- TensorFlow, PyTorch\\n- Problem-Solving\\n- Analytical Skills\\n- Communication and Collaboration\\n\\n---\\n\\n**Certifications:**\\n\\n- Certified TensorFlow Developer\\n- Machine Learning with Python by Coursera\\n\\n---\\n\\n**Projects:**\\n\\n- **AI-Based Customer Service Chatbot:** Developed and deployed an NLP model that improved customer response accuracy by 30%.\\n- **Image Recognition System:** Implemented a neural network model for image recognition, achieving 95% accuracy.\\n\\n---\\n\\n**Languages:**\\n\\n- English (Fluent)\\n- Urdu (Native)\\n\\n---\\n\\n**References:**\\n\\nAvailable upon request\",\n",
       " 'source': 'some',\n",
       " 'selection_Bool': [predict_bool(score='0.95')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "import os\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<job description>\n",
    "{job_des}\n",
    "</job description>\n",
    "------------\n",
    "<cv>\n",
    "{cv}\n",
    "</cv>\n",
    "\n",
    "> Relevant (YES / NO):\n",
    "\"\"\"\n",
    "\n",
    "template = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"job_des\", \"cv\"])\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class predict_bool(BaseModel):\n",
    "    \"\"\"You are a powerful HR assistant. Your task is to review the given CV and determine if it matches the job requirements specified in the job description and give socre between 0 and 1 based on their relvancy.Please do your best it is very important to my career.if both or any of feild is empty then also return 0.\n",
    "    Also give return the matching score betweeen 0 and 1\n",
    "    > Relevant Score Between (0 and 1):\n",
    "    \"\"\"\n",
    "\n",
    "    # selection: str = Field(..., description=\"YES/NO\")\n",
    "    score: str = Field(..., description=\"Give the score to the cv between 0 and 1\")\n",
    "\n",
    "google = ChatGoogleGenerativeAI(max_retries=0,temperature=0,model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\"))\n",
    "\n",
    "groq = ChatGroq(temperature=0,max_retries=0,model_name=\"mixtral-8x7b-32768\").with_fallbacks([google])\n",
    "\n",
    "groq2 = ChatGroq(api_key=\"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\",temperature=0,max_retries=0,model_name=\"mixtral-8x7b-32768\").with_fallbacks([groq])\n",
    "\n",
    "llm_with_tools = groq.bind_tools([predict_bool],tool_choice=\"predict_bool\")\n",
    "\n",
    "job_des_cv_chain = RunnablePassthrough.assign(\n",
    "selection_Bool = RunnablePassthrough.assign(\n",
    "    source = (lambda x:x[\"source\"]),\n",
    "    job_des = (lambda x:x[\"job_des\"]),\n",
    "    cv = (lambda x:x[\"cv\"])) | template | llm_with_tools | PydanticToolsParser(tools=[predict_bool]))\n",
    "\n",
    "from utils import r_cv,ir_cv\n",
    "\n",
    "# job_des_cv_chain.invoke({\"job_des\":des,\"cv\":r_cv,\"source\":\"some\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import os,asyncio,warnings,uuid\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def flatteningdocs(data):\n",
    "    # des = \"we are looing for python developer just no more\"\n",
    "    flatten =  [item for sublist in data for item in sublist]\n",
    "    return [{\"source\":cv.metadata['source'],\"cv\":cv.page_content,\"job_des\":des} for cv in flatten]\n",
    "\n",
    "def get_data(file_path):\n",
    "    try:\n",
    "        pages = UnstructuredFileLoader(file_path=file_path).load()\n",
    "        if pages[0].page_content:\n",
    "            return pages\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "pdfs_dir = \"data/resumes/8cb73b16-615b-47d4-8c14-5735cd3bebfd\"\n",
    "\n",
    "pdfs_path = [f\"{pdfs_dir}/{pdf}\" for pdf in os.listdir(pdfs_dir)]\n",
    "\n",
    "content_chain = RunnablePassthrough() | get_data\n",
    "pdfs_content = flatteningdocs(await content_chain.abatch(pdfs_path[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortlist_cvs(cv_list, percentage):\n",
    "    scored_cvs = [(cv.get(\"source\"),cv.get(\"selection_Bool\")[0].score) for cv in cv_list]\n",
    "    \n",
    "    # Sort CVs based on relevance scores in descending order\n",
    "    scored_cvs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate the number of CVs to shortlist based on the percentage\n",
    "    shortlist_count = int(len(cv_list) * percentage / 100)\n",
    "    \n",
    "    # Select the top N percent CVs\n",
    "    shortlisted_cvs = scored_cvs[:shortlist_count]\n",
    "    \n",
    "    return shortlisted_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = job_des_cv_chain.batch(pdfs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data/resumes/8cb73b16-615b-47d4-8c14-5735cd3bebfd/Python Developer78.pdf',\n",
       "  '0.65'),\n",
       " ('data/resumes/8cb73b16-615b-47d4-8c14-5735cd3bebfd/Python Developer76.pdf',\n",
       "  '0.6'),\n",
       " ('data/resumes/8cb73b16-615b-47d4-8c14-5735cd3bebfd/Python Developer74.pdf',\n",
       "  '0.5')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithConnectionError('Connection error caused failure to POST https://api.smith.langchain.com/runs/batch  in LangSmith API. Please confirm your internet connection.. SSLError(MaxRetryError(\"HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/batch (Caused by SSLError(SSLEOFError(8, \\'EOF occurred in violation of protocol (_ssl.c:2406)\\')))\"))')\n",
      "Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "shortlist_cvs(res,percentage=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm an AI and I don't have feelings, but I'm here to help you. How can I assist you today?\\n\\n(To use a tool, return a JSON object as a JSON string that matches the tool_use structure provided in the instructions. If no tool is needed, simply respond directly.)\", response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 1245, 'total_tokens': 1315, 'completion_time': 0.111389901, 'prompt_time': 0.092694035, 'queue_time': None, 'total_time': 0.204083936}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ca2b9983-aa20-4718-8cfc-cd77d7e155b8-0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class predict_bool(BaseModel):\n",
    "    \"\"\"You are a powerful HR assistant. Your task is to review the given CV and determine if it matches the job requirements specified in the job description. Return only \"YES\" if it matches the requirements; otherwise, return \"NO\".Please do your best it is very important to my career.if both or any of feild is empty then also return NO.\"\"\"\n",
    "\n",
    "    selection: str = Field(..., description=\"Yes/No\")\n",
    "\n",
    "llm_with_tools = model.bind_tools([predict_bool])\n",
    "\n",
    "llm_with_tools.invoke(\"HI how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 22:27:12.100317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load a pre-trained language model for zero-shot classification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-mnli\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_cv\u001b[39m(job_description, cv_text):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Define the labels\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot relevant\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:3351\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3337\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3350\u001b[0m     }\n\u001b[0;32m-> 3351\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3353\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3354\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3356\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    939\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained language model for zero-shot classification\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "def score_cv(job_description, cv_text):\n",
    "    # Define the labels\n",
    "    labels = [\"relevant\", \"not relevant\"]\n",
    "    # Classify the CV against the job description\n",
    "    result = classifier(cv_text, candidate_labels=labels, hypothesis_template=f\"This CV is {labels[0]} for the job description.\")\n",
    "    # Return the relevance score for \"relevant\"\n",
    "    relevance_score = result[\"scores\"][result[\"labels\"].index(\"relevant\")]\n",
    "    return relevance_score\n",
    "\n",
    "def shortlist_cvs(job_description, cv_list, percentage):\n",
    "    scored_cvs = []\n",
    "    \n",
    "    for cv in cv_list:\n",
    "        score = score_cv(job_description, cv)\n",
    "        scored_cvs.append((cv, score))\n",
    "    \n",
    "    # Sort CVs based on relevance scores in descending order\n",
    "    scored_cvs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate the number of CVs to shortlist based on the percentage\n",
    "    shortlist_count = int(len(cv_list) * percentage / 100)\n",
    "    \n",
    "    # Select the top N percent CVs\n",
    "    shortlisted_cvs = scored_cvs[:shortlist_count]\n",
    "    \n",
    "    return shortlisted_cvs\n",
    "\n",
    "# Example usage\n",
    "job_description = \"Job description text here...\"\n",
    "cv_list = [\"CV text 1...\", \"CV text 2...\", \"CV text 3...\"]\n",
    "\n",
    "# Shortlist the top 10% of CVs\n",
    "percentage = 10\n",
    "shortlisted_cvs = shortlist_cvs(job_description, cv_list, percentage)\n",
    "\n",
    "# Output the shortlisted CVs and their scores\n",
    "for cv, score in shortlisted_cvs:\n",
    "    print(f\"CV: {cv}, Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
