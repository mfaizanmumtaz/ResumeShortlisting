{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "import os,asyncio,warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = [file for file in os.listdir('data') if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "pages = PyPDFLoader(file_path=f'/home/faizan/Downloads/Get_Started_With_Smallpdf.pdf').load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatteningdocs(data):\n",
    "    return [item for sublist in data for item in sublist]\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "def get_data(file_path):\n",
    "    pages = PyPDFLoader(file_path=f'data/{file_path}').load()\n",
    "    if len(pages) > 1:\n",
    "        pdfstring = \"\"\n",
    "        metadata = {}\n",
    "        for page in pages:\n",
    "            pdfstring += page.page_content\n",
    "            metadata.update(page.metadata)\n",
    "\n",
    "        return [Document(\n",
    "            page_content=pdfstring,\n",
    "            metadata=metadata)]\n",
    "        \n",
    "    else:\n",
    "        return pages\n",
    "\n",
    "content_chain = RunnablePassthrough() | get_data\n",
    "content = flatteningdocs(await content_chain.abatch(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/UpdatedResumeDataSet.csv')\n",
    "# resumes = df['Resume'].tolist()\n",
    "resumes = df[(df['Category'] == \"Database\") | (df['Category'] == \"Data Science\") | (df['Category'] == 'Python Developer')]\n",
    "\n",
    "resumes.drop_duplicates(inplace=True)\n",
    "resumes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "data_tuples = [tuple(x) for x in resumes.to_records(index=False)]\n",
    "\n",
    "def save_to_pdf(data):\n",
    "    for index,(category, resume_text) in enumerate(data):\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font('Arial', size=10)\n",
    "        pdf.set_title(f\"{category}.pdf\")\n",
    "        \n",
    "        # Encode resume_text to 'latin-1' to handle Unicode characters\n",
    "        try:\n",
    "            encoded_text = resume_text.encode('latin-1', 'replace').decode('latin-1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding text for {category}.pdf: {e}\")\n",
    "            continue\n",
    "        \n",
    "        pdf.multi_cell(0, 10, encoded_text, border=1)\n",
    "        \n",
    "        # Save the PDF file\n",
    "        pdf_file_name = f\"data/{category}{index + 1}.pdf\"\n",
    "        pdf.output(pdf_file_name)\n",
    "\n",
    "save_to_pdf(data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = \"\"\"Company Description\n",
    " \n",
    "Evergreen Future Tech (EFT) is a software solutions provider located in Islamabad, Pakistan. We offer cutting-edge software solutions and specialize in artificial intelligence technologies.\n",
    "\n",
    " Role Description\n",
    " \n",
    "This is a contract role for an Artificial Intelligence Engineer. The AI Engineer will be responsible for developing and implementing AI technologies, such as pattern recognition, neural networks, and natural language processing. The engineer will also be involved in software development and computer science-related tasks. This role is an on-site position located in Islamabad, Pakistan.\n",
    "\n",
    " Qualifications\n",
    " \n",
    "Strong background in computer science and software development\n",
    "Proficiency in pattern recognition and neural networks\n",
    "Experience in natural language processing (NLP)\n",
    "Problem-solving and analytical skills\n",
    "Experience with AI frameworks and tools\n",
    "Knowledge of machine learning algorithms\n",
    "Excellent coding skills in languages such as Python or Java\n",
    "Strong communication and collaboration skills\n",
    "Bachelor's or master's degree in computer science or a related field\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "des2 = \"we are looking for a doctor who is best in his feild mostly in the animle and he should be experinced.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "# vector_store = Chroma.from_texts(embedding=embedding,texts=resumes)\n",
    "persist_directory = \"database\"\n",
    "vector_store = Chroma.from_documents(embedding=embedding,documents=content,persist_directory=persist_directory)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_kwargs={'k':20})\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "        # search_kwargs={'score_threshold': 0.1,'k':30})\n",
    "# print(len(getunique(retriever.invoke(des))))\n",
    "len(retriever.invoke(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'source': 'data/Data Science8.pdf'}, page_content='Education Details \\n B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology\\nData Science \\nData Science\\nSkill Details \\nNumpy- Exprience - Less than 1 year months\\nMachine Learning- Exprience - Less than 1 year months\\nTensorflow- Exprience - Less than 1 year months\\nScikit- Exprience - Less than 1 year months\\nPython- Exprience - Less than 1 year months\\nGCP- Exprience - Less than 1 year months\\nPandas- Exprience - Less than 1 year months\\nNeural Network- Exprience - Less than 1 year monthsCompany Details \\ncompany - Wipro\\ndescription - Bhawana Aggarwal\\nE-Mail:bhawana.chd@gmail.com\\nPhone: 09876971076\\nVVersatile, high-energy professional targeting challenging assignments in Machine\\nPROFILE SUMMARY\\nâ–ª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine\\nLearning, Deep Learning, Data Science, Python, Software Development.\\nâ–ª Skilled in managing end-to-end development and software products / projects from inception, requirement\\nspecs, planning, designing, implementation, configuration and documentation.\\nâ–ª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,\\nNLP, GCP.â–ª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.\\nâ–ª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,\\nSupport vector Machine(SVM),Logistic Regression, Neural networks.\\nâ–ª Have knowledge on unsupervised, Supervised and reinforcement data.\\nâ–ª Programming experience in relational platforms like MySQL,Oracle.\\nâ–ª Have knowledge on Some programming language like C++,Java.\\nâ–ª Experience in cloud based environment like Google Cloud.\\nâ–ª Working on different Operating System like Linux, Ubuntu, Windows.\\nâ–ª Good interpersonal and communication skills.\\nâ–ª Problem solving skills with the ability to think laterally, and to think with a medium term and long term\\nperspective\\nâ–ª Flexibility and an open attitude to change.\\nâ–ª Ability to create, define and own frameworks with a strong emphasis on code reusability.\\nTECHNICAL SKILLS\\nProgramming Languages Python, C\\nLibraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib\\nAlgorithms\\nKNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,\\nTensorflow, SVM\\nDatabases SQL, Oracle\\nOperating Systems Linux, Window\\nDevelopment Environments NetBeans, Notebooks, Sublime\\nTicketing tools Service Now, Remedy\\nEducation\\nUG Education:\\nB.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in2016.\\nSchooling:\\nXII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)\\nX in 2010 from Valley Public School (Passed with 9.4 CGPA)\\nWORK EXPERINCE\\nTitle : Wipro Neural Intelligence Platform\\nTeam Size : 5\\nBrief: Wiproâ€™s Neural Intelligence Platform harnesses the power of automation and artificial intelligence\\ntechnologiesâ€”natural language processing (NLP), cognitive, machine learning, and analytics. The platform\\ncomprises three layers: a data engagement platform that can easily access and manage multiple structured and\\nunstructured data sources; an â€œintent assessment and reasoningâ€\\x9d engine that includes sentiment and predictive\\nanalytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed\\nautomating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,\\nNLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be\\neliminated.\\nEntity Extractor -> This involves text extraction and NLP for fetching out important information from the text like\\ndates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâ€™s and machine learning using\\nTensor flow for further learning of new entities.\\nClassifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn\\nclassifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best\\nsuited response and make the system efficient.\\nNER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,\\nOrganizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities\\nusing Keras TensorFlow framework.\\nOTHER PROJECTS\\nTitle : Diabetes DetectionBrief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third\\nprize in it.\\nTRAINING AND CERTIFICATIONS\\nTitle: Python Training, Machine Learning, Data Science, Deep Learning\\nOrganization: Udemy, Coursera (Machine Learning, Deep Learning)\\nPersonal Profile\\nFatherâ€™s Name :Mr. Tirlok Aggarwal\\nLanguage Known : English & Hindi\\nMarital Status :Single\\nDate of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)\\ncompany - Wipro\\ndescription - Developing programs in Python.\\ncompany - Wipro\\ndescription - Title : Wipro Neural Intelligence Platform\\nTeam Size : 5\\nBrief: Wiproâ€™s Neural Intelligence Platform harnesses the power of automation and artificial intelligence\\ntechnologiesâ€”natural language processing (NLP), cognitive, machine learning, and analytics. The platform\\ncomprises three layers: a data engagement platform that can easily access and manage multiple structured and\\nunstructured data sources; an â€œintent assessment and reasoningâ€\\x9d engine that includes sentiment and predictive\\nanalytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed\\nautomating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,\\nNLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be\\neliminated.\\nEntity Extractor -> This involves text extraction and NLP for fetching out important information from the text like\\ndates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâ€™s and machine learning using\\nTensor flow for further learning of new entities.Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn\\nclassifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best\\nsuited response and make the system efficient.\\nNER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,\\nOrganizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities\\nusing Keras TensorFlow framework.\\ncompany - Wipro Technologies\\ndescription - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine\\nLearning, Deep Learning, Data Science, Python, Software Development.\\nâ–ª Skilled in managing end-to-end development and software products / projects from inception, requirement\\nspecs, planning, designing, implementation, configuration and documentation.\\nâ–ª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,\\nNLP, GCP.\\nâ–ª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.\\nâ–ª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,\\nSupport vector Machine(SVM),Logistic Regression, Neural networks.\\nâ–ª Have knowledge on unsupervised, Supervised and reinforcement data.\\nâ–ª Programming experience in relational platforms like MySQL,Oracle.\\nâ–ª Have knowledge on Some programming language like C++,Java.\\nâ–ª Experience in cloud based environment like Google Cloud.\\nâ–ª Working on different Operating System like Linux, Ubuntu, Windows.\\nâ–ª Good interpersonal and communication skills.\\nâ–ª Problem solving skills with the ability to think laterally, and to think with a medium term and long term\\nperspective\\nâ–ª Flexibility and an open attitude to change.\\nâ–ª Ability to create, define and own frameworks with a strong emphasis on code reusability.'),\n",
       " Document(metadata={'page': 1, 'source': 'data/Data Science7.pdf'}, page_content='Skills â€¢ Python â€¢ Tableau â€¢ Data Visualization â€¢ R Studio â€¢ Machine Learning â€¢ Statistics IABAC\\nCertified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and\\nleading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of\\naugmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business\\nconcepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details \\nJanuary 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering\\nData Science Consultant \\nData Science Consultant - Datamites\\nSkill Details \\nMACHINE LEARNING- Exprience - 13 months\\nPYTHON- Exprience - 24 months\\nSOLUTIONS- Exprience - 24 months\\nDATA SCIENCE- Exprience - 24 months\\nDATA VISUALIZATION- Exprience - 24 months\\nTableau- Exprience - 24 monthsCompany Details \\ncompany - Datamites\\ndescription - â€¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.\\nâ€¢ Responsible for loading, extracting and validation of client data.\\nâ€¢ Worked on manipulating, cleaning & processing data using python.\\nâ€¢ Used Tableau for data visualization.\\ncompany - Heretic Solutions Pvt Ltd\\ndescription - â€¢ Worked closely with business to identify issues and used data to propose solutions for effective\\ndecision making.\\nâ€¢ Manipulating, cleansing & processing data using Python, Excel and R.\\nâ€¢ Analyzed raw data, drawing conclusions & developing recommendations.â€¢ Used machine learning tools and statistical techniques to produce solutions to problems.'),\n",
       " Document(metadata={'page': 1, 'source': 'data/Data Science9.pdf'}, page_content='Personal Skills âž¢ Ability to quickly grasp technical aspects and willingness to learn âž¢ High energy levels & Result\\noriented. Education Details \\nJanuary 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of\\nEngineering & Information Technology\\nJanuary 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of\\nEngineering\\nJanuary 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha\\nJanuary 2003 M.tech Thesis Detail  BMCH School in Ganj basoda\\nData science \\nI have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP,\\nPython, SQL, Web Scraping Good knowledge in computer subjects and ability to update\\nSkill Details \\nExperience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than\\n1 year monthsCompany Details \\ncompany - RNT.AI Technology Solution\\ndescription - Text classification using Machine learning Algorithms with python.\\nPractical knowledge of Deep learning algorithms such as Â\\xa0Recurrent Neural Networks(RNN).\\nDevelop custom data models and algorithms to apply to dataset\\nExperience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.\\nComfort with SQL, Â\\xa0MYSQL\\nSentiment analysis.\\nÂ\\xa0Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning\\nAlgorithms.\\nWeb crawling using Selenium web driver and Beautiful Soup with python.\\ncompany - Life Insurance Corporation of India Bhopaldescription - Ã¼Â\\xa0Explaining policy features and the benefits\\nÃ¼ Updated knowledge of life insurance products and shared with customers'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Data Science10.pdf'}, page_content='Expertise âˆ’ Data and Quantitative Analysis âˆ’ Decision Analytics âˆ’ Predictive Modeling âˆ’ Data-Driven\\nPersonalization âˆ’ KPI Dashboards âˆ’ Big Data Queries and Interpretation âˆ’ Data Mining and Visualization Tools âˆ’\\nMachine Learning Algorithms âˆ’ Business Intelligence (BI) âˆ’ Research, Reports and Forecasts Education Details \\n PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business\\n B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy\\nData Scientist \\nData Scientist with PR Canada\\nSkill Details \\nAlgorithms- Exprience - 6 months\\nBI- Exprience - 6 months\\nBusiness Intelligence- Exprience - 6 months\\nMachine Learning- Exprience - 24 months\\nVisualization- Exprience - 24 months\\nspark- Exprience - 24 months\\npython- Exprience - 36 months\\ntableau- Exprience - 36 months\\nData Analysis- Exprience - 24 monthsCompany Details \\ncompany - Aegis school of Data Science & Business\\ndescription - Mostly working on industry project for providing solution along with Teaching Appointments: Teach\\nundergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis\\nSchool of Data Science, Mumbai (2017 to Present)\\ncompany - Aegis school of Data & Business\\ndescription - Data Science Intern, Nov 2015 to Jan 2016\\nFurnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategicplanning across all business units, distribution channels and product lines.\\nâž” Chat Bot using AWS LEX and Tensor flow  Python\\nThe goal of project creates a chat bot for an academic institution or university to handle queries related courses offered\\nby that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this\\ncompanies handle their client 24x7. In this case companies are academic institutions and clients are participants or\\nstudents.\\nâž” Web scraping using Selenium web driver   Python\\nThe task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.\\nâž” Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense\\nThe goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which\\nis generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate\\na reports.\\nâž” Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV\\nThe main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing\\na special face from set of different faces. Face is extracted and then compared with the database Image if that Image\\nrecognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how\\nwe are going to prevent the frauds in the initial stage itself.\\nâž” Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop\\nThe objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer\\ngiving incentive offers.\\nâž” Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a\\nsentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions\\nof the tweets and comparative analysis.Quantifiable Results:\\nâˆ’ Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science\\nand Business Analytics.\\nâˆ’ Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference\\ncalled Data Science Congress by Aegis School of Business Mumbai.\\nâˆ’ Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai\\nand managed 4-5 live projects using Data Science techniques.\\nâˆ’ Working for some social cause with the help of Data Science for Social Goods Committee, where our team\\ndeveloped a product called \"Let\\'s find a missing Child\" for helping society.\\ncompany - IBM India pvt ltd\\ndescription - Mostly worked on blumix and IBM Watson for Data science.'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Data Science1.pdf'}, page_content='Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery.\\n* Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques,\\nCluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic\\nModelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase,\\nElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS,\\nAngular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep\\nlearning.Education Details \\nData Science Assurance Associate \\nData Science Assurance Associate - Ernst & Young LLP\\nSkill Details \\nJAVASCRIPT- Exprience - 24 months\\njQuery- Exprience - 24 months\\nPython- Exprience - 24 monthsCompany Details \\ncompany - Ernst & Young LLP\\ndescription - Fraud Investigations and Dispute Services   Assurance\\nTECHNOLOGY ASSISTED REVIEW\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery\\ndomain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor\\ncosts and time spent during the lawyers review.\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive\\nanalysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring\\nfor the entire tool.\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifiermodels in order to identify \"red flags\" and fraud-related issues.\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for\\ntopic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for\\npast one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments\\nacross all 4 categories.\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words\\nacross all the Survey categories and plotted Word cloud.\\n* Created customized tableau dashboards for effective reporting and visualizations.\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of\\noperation, reservation options and so on.\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give\\nrecommendation responses so that user question to build chain of relevant answer.\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant\\n/recommended questions.\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word\\nEmbedding, scikit-learn, JavaScript/JQuery, SqlServer\\nINFORMATION GOVERNANCE\\nOrganizations to make informed decisions about all of the information they store. The integrated Information\\nGovernance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure\\norganizations are best positioned to counter information risk.* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results\\nfor indexing elastic search and created customized, interactive dashboards using kibana.\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either\\nRedundant, Outdated, or Trivial.\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally\\nidentifiable information (social security numbers, addresses, names, etc.) which frequently targeted during\\ncyber-attacks.\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\nFRAUD ANALYTIC PLATFORM\\nFraud Analytics and investigative platform to review all red flag cases.\\nâ€¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP\\nsystems.\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators\\nof fraud by running advanced analytics\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"CV\"\n",
    "\n",
    "Groq = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\").with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\"))])\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\")).with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"dgoogle_api_key\"))]).with_fallbacks([Groq])\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "\n",
    "prompt_template = \"\"\"You are a powerfull assistant your task is to check weather the given CV match the given job requirements.Return only Yes if it match else return No.\n",
    "<job description>{question}</job description>\n",
    "# <cv>{context}</cv>\n",
    "> Relevant (YES / NO):\"\"\"\n",
    "\n",
    "def _get_default_chain_prompt() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        output_parser=BooleanOutputParser())\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(Groq,prompt=_get_default_chain_prompt())\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(des)\n",
    "\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # len(getunique(compressed_docs))\n",
    "# from langchain.load import dumps,loads\n",
    "# def getunique(docs):\n",
    "#     # return [loads(_doc) for _doc in set([dumps(doc) for doc in docs])]\n",
    "#     return [loads(_doc) for _doc in (set(dumps(doc) for doc in docs))]\n",
    "\n",
    "# for doc in getunique(compressed_docs):\n",
    "#     print(doc.metadata)\n",
    "    \n",
    "# # for doc in compressed_docs:\n",
    "# # compressed_docs\n",
    "\n",
    "import shutil\n",
    "shortlisted_cvs = [cv.metadata['source'] for cv in compressed_docs]\n",
    "for filename in shortlisted_cvs:\n",
    "    shutil.copy(filename, 'shortlisted_cvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(max_retries=0,model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\")).with_fallbacks([ChatGoogleGenerativeAI(max_retries=0,model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"dgoogle_api_key\"))]).with_fallbacks([Groq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.remove(f'shortlisted_cvs/{file}') for file in os.listdir('shortlisted_cvs')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # Install package\n",
    "# %pip install --upgrade --quiet \"unstructured[all-docs]\" --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% of 27 is 18\n"
     ]
    }
   ],
   "source": [
    "def find_percent(number):\n",
    "    return int(number * 0.70)\n",
    "\n",
    "# Example usage:\n",
    "num = 27\n",
    "result = find_percent(num)\n",
    "print(f\"70% of {num} is {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import os,asyncio,warnings\n",
    "from langchain_chroma import Chroma\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "persist_directory = \"db2\"\n",
    "\n",
    "vector_store = Chroma.from_documents(persist_directory=persist_directory,collection_name=\"faizan1\",documents=[Document(page_content=\"Hello, I'm a document!\"),Document(page_content=\"This is a second document!\")],embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    return Chroma.from_documents(persist_directory=persist_directory,collection_name=\"faizan3\",documents=[Document(page_content=\"we are pakistan\"),Document(page_content=\"we are muslim\"),Document(page_content=\"I am faizan mumtaz\"),Document(page_content=\"we will fight\"),Document(page_content=\"Muhammad Faizan Mumtaz is ML engineer.\"),Document(page_content=\"\")],embedding=embedding)\n",
    "\n",
    "r = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='we are pakistan'),\n",
       " Document(page_content='we are pakistan'),\n",
       " Document(page_content='we are muslim'),\n",
       " Document(page_content='we are muslim'),\n",
       " Document(page_content='I am faizan mumtaz'),\n",
       " Document(page_content='I am faizan mumtaz'),\n",
       " Document(page_content='I am faizan mumtaz'),\n",
       " Document(page_content='we will fight'),\n",
       " Document(page_content='we will fight'),\n",
       " Document(page_content='Muhammad Faizan Mumtaz is ML engineer.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r.delete_collection()\n",
    "r.as_retriever(search_kwargs={\"k\":10}).invoke('we are pakis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(persist_directory=persist_directory,collection_name=\"faizan1\",embedding_function=embedding)\n",
    "# vector_store.invoke(\"second document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=faizan1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faizan/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-07-06 09:28:49.827754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/faizan/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 in the collection\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "langchain_chroma = Chroma(\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f10c37bf-f232-4d1e-911b-6afd3b5015a3.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "zip_filename = str(uuid.uuid4()) + \".zip\"\n",
    "zip_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('test',exist_ok=True)\n",
    "os.makedirs(\"test/faizan\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"test/faizan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
