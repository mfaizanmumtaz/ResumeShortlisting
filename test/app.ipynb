{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "import os,asyncio,warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = [file for file in os.listdir('data') if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "pages = PyPDFLoader(file_path=f'/home/faizan/Downloads/Get_Started_With_Smallpdf.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatteningdocs(data):\n",
    "    return [item for sublist in data for item in sublist]\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "def get_data(file_path):\n",
    "    pages = PyPDFLoader(file_path=f'data/{file_path}').load()\n",
    "    if len(pages) > 1:\n",
    "        pdfstring = \"\"\n",
    "        metadata = {}\n",
    "        for page in pages:\n",
    "            pdfstring += page.page_content\n",
    "            metadata.update(page.metadata)\n",
    "\n",
    "        return [Document(\n",
    "            page_content=pdfstring,\n",
    "            metadata=metadata)]\n",
    "        \n",
    "    else:\n",
    "        return pages\n",
    "\n",
    "content_chain = RunnablePassthrough() | get_data\n",
    "content = flatteningdocs(await content_chain.abatch(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('UpdatedResumeDataSet.csv')\n",
    "# resumes = df['Resume'].tolist()\n",
    "resumes = df[(df['Category'] == \"Database\") | (df['Category'] == \"Data Science\") | (df['Category'] == 'Python Developer')]\n",
    "\n",
    "resumes.drop_duplicates(inplace=True)\n",
    "resumes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "data_tuples = [tuple(x) for x in resumes.to_records(index=False)]\n",
    "\n",
    "def save_to_pdf(data):\n",
    "    for index,(category, resume_text) in enumerate(data):\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font('Arial', size=10)\n",
    "        pdf.set_title(f\"{category}.pdf\")\n",
    "        \n",
    "        # Encode resume_text to 'latin-1' to handle Unicode characters\n",
    "        try:\n",
    "            encoded_text = resume_text.encode('latin-1', 'replace').decode('latin-1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding text for {category}.pdf: {e}\")\n",
    "            continue\n",
    "        \n",
    "        pdf.multi_cell(0, 10, encoded_text, border=1)\n",
    "        \n",
    "        # Save the PDF file\n",
    "        pdf_file_name = f\"data/{category}{index + 1}.pdf\"\n",
    "        pdf.output(pdf_file_name)\n",
    "\n",
    "save_to_pdf(data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "# vector_store = Chroma.from_texts(embedding=embedding,texts=resumes)\n",
    "persist_directory = \"database\"\n",
    "vector_store = Chroma.from_documents(embedding=embedding,documents=content,persist_directory=persist_directory)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_kwargs={'k':20})\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "        # search_kwargs={'score_threshold': 0.1,'k':30})\n",
    "# print(len(getunique(retriever.invoke(des))))\n",
    "len(retriever.invoke(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"CV\"\n",
    "\n",
    "Groq = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\").with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\"))])\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\")).with_fallbacks([ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"dgoogle_api_key\"))]).with_fallbacks([Groq])\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "\n",
    "prompt_template = \"\"\"You are a powerfull assistant your task is to check weather the given CV match the given job requirements.Return only Yes if it match else return No.\n",
    "<job description>{question}</job description>\n",
    "# <cv>{context}</cv>\n",
    "> Relevant (YES / NO):\"\"\"\n",
    "\n",
    "def _get_default_chain_prompt() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        output_parser=BooleanOutputParser())\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(Groq,prompt=_get_default_chain_prompt())\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(des)\n",
    "\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.remove(f'shortlisted_cvs/{file}') for file in os.listdir('shortlisted_cvs')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install package\n",
    "# %pip install --upgrade --quiet \"unstructured[all-docs]\" --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('test',exist_ok=True)\n",
    "os.makedirs(\"test/faizan\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"test/faizan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "import os\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<job description>\n",
    "{job_des}\n",
    "</job description>\n",
    "------------\n",
    "<cv>\n",
    "{cv}\n",
    "</cv>\n",
    "\n",
    "> Relevant (YES / NO):\n",
    "\"\"\"\n",
    "\n",
    "template = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"job_des\", \"cv\"])\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class predict_bool(BaseModel):\n",
    "    \"\"\"You are a powerful HR assistant. Your task is to review the given CV and determine if it matches the job requirements specified in the job description and give socre between 0 and 1 based on their relvancy.Please do your best it is very important to my career.if both or any of feild is empty then also return 0.\n",
    "    Also give return the matching score betweeen 0 and 1\n",
    "    > Relevant Score Between (0 and 1):\n",
    "    \"\"\"\n",
    "\n",
    "    # selection: str = Field(..., description=\"YES/NO\")\n",
    "    score: str = Field(..., description=\"Give the score to the cv between 0 and 1\")\n",
    "\n",
    "google = ChatGoogleGenerativeAI(max_retries=0,temperature=0,model=\"gemini-1.5-flash\",google_api_key=os.getenv(\"google_api_key\"))\n",
    "\n",
    "groq = ChatGroq(temperature=0,max_retries=0,model_name=\"mixtral-8x7b-32768\").with_fallbacks([google])\n",
    "\n",
    "groq2 = ChatGroq(api_key=\"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\",temperature=0,max_retries=0,model_name=\"mixtral-8x7b-32768\").with_fallbacks([groq])\n",
    "\n",
    "llm_with_tools = groq.bind_tools([predict_bool],tool_choice=\"predict_bool\")\n",
    "\n",
    "job_des_cv_chain = RunnablePassthrough.assign(\n",
    "selection_Bool = RunnablePassthrough.assign(\n",
    "    source = (lambda x:x[\"source\"]),\n",
    "    job_des = (lambda x:x[\"job_des\"]),\n",
    "    cv = (lambda x:x[\"cv\"])) | template | llm_with_tools | PydanticToolsParser(tools=[predict_bool]))\n",
    "\n",
    "from utils import r_cv,ir_cv\n",
    "\n",
    "# job_des_cv_chain.invoke({\"job_des\":des,\"cv\":r_cv,\"source\":\"some\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import os,asyncio,warnings,uuid\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def flatteningdocs(data):\n",
    "    # des = \"we are looing for python developer just no more\"\n",
    "    flatten =  [item for sublist in data for item in sublist]\n",
    "    return [{\"source\":cv.metadata['source'],\"cv\":cv.page_content,\"job_des\":des} for cv in flatten]\n",
    "\n",
    "def get_data(file_path):\n",
    "    try:\n",
    "        pages = UnstructuredFileLoader(file_path=file_path).load()\n",
    "        if pages[0].page_content:\n",
    "            return pages\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "pdfs_dir = \"data/resumes/8cb73b16-615b-47d4-8c14-5735cd3bebfd\"\n",
    "\n",
    "pdfs_path = [f\"{pdfs_dir}/{pdf}\" for pdf in os.listdir(pdfs_dir)]\n",
    "\n",
    "content_chain = RunnablePassthrough() | get_data\n",
    "pdfs_content = flatteningdocs(await content_chain.abatch(pdfs_path[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortlist_cvs(cv_list, percentage):\n",
    "    scored_cvs = [(cv.get(\"source\"),cv.get(\"selection_Bool\")[0].score) for cv in cv_list]\n",
    "    \n",
    "    # Sort CVs based on relevance scores in descending order\n",
    "    scored_cvs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate the number of CVs to shortlist based on the percentage\n",
    "    shortlist_count = int(len(cv_list) * percentage / 100)\n",
    "    \n",
    "    # Select the top N percent CVs\n",
    "    shortlisted_cvs = scored_cvs[:shortlist_count]\n",
    "    \n",
    "    return shortlisted_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = job_des_cv_chain.batch(pdfs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlist_cvs(res,percentage=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class predict_bool(BaseModel):\n",
    "    \"\"\"You are a powerful HR assistant. Your task is to review the given CV and determine if it matches the job requirements specified in the job description. Return only \"YES\" if it matches the requirements; otherwise, return \"NO\".Please do your best it is very important to my career.if both or any of feild is empty then also return NO.\"\"\"\n",
    "\n",
    "    selection: str = Field(..., description=\"Yes/No\")\n",
    "\n",
    "llm_with_tools = model.bind_tools([predict_bool])\n",
    "\n",
    "llm_with_tools.invoke(\"HI how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained language model for zero-shot classification\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "def score_cv(job_description, cv_text):\n",
    "    # Define the labels\n",
    "    labels = [\"relevant\", \"not relevant\"]\n",
    "    # Classify the CV against the job description\n",
    "    result = classifier(cv_text, candidate_labels=labels, hypothesis_template=f\"This CV is {labels[0]} for the job description.\")\n",
    "    # Return the relevance score for \"relevant\"\n",
    "    relevance_score = result[\"scores\"][result[\"labels\"].index(\"relevant\")]\n",
    "    return relevance_score\n",
    "\n",
    "def shortlist_cvs(job_description, cv_list, percentage):\n",
    "    scored_cvs = []\n",
    "    \n",
    "    for cv in cv_list:\n",
    "        score = score_cv(job_description, cv)\n",
    "        scored_cvs.append((cv, score))\n",
    "    \n",
    "    # Sort CVs based on relevance scores in descending order\n",
    "    scored_cvs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate the number of CVs to shortlist based on the percentage\n",
    "    shortlist_count = int(len(cv_list) * percentage / 100)\n",
    "    \n",
    "    # Select the top N percent CVs\n",
    "    shortlisted_cvs = scored_cvs[:shortlist_count]\n",
    "    \n",
    "    return shortlisted_cvs\n",
    "\n",
    "# Example usage\n",
    "job_description = \"Job description text here...\"\n",
    "cv_list = [\"CV text 1...\", \"CV text 2...\", \"CV text 3...\"]\n",
    "\n",
    "# Shortlist the top 10% of CVs\n",
    "percentage = 10\n",
    "shortlisted_cvs = shortlist_cvs(job_description, cv_list, percentage)\n",
    "\n",
    "# Output the shortlisted CVs and their scores\n",
    "for cv, score in shortlisted_cvs:\n",
    "    print(f\"CV: {cv}, Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"MachineLearning.pdf\")\n",
    "data = loader.load()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"MachineLearning.pdf\")\n",
    "data = loader.load()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma.from_documents(\n",
    "documents=data,\n",
    "embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2\")\n",
    "model.invoke(\"HI what are you doing right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model = \"gemma2\"\n",
    ")\n",
    "llm.invoke(\"Tell me a joke?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")  # by default, uses llama2. Run `ollama pull llama2` to pull down the model\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "embeddings.embed_query(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
